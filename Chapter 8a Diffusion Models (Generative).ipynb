{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ef4fcd",
   "metadata": {},
   "source": [
    "Diffusion Models are generative models. These means that the are used to generate data which happens to be similar to the data on what they are trained for. The main idea is that the model destroys the training data by adding Gaussian noise and then, it learns how to recoved the data by reversing the noising process. If the process is well developed, the diffusion model is able to generate data by just receiving a simple sampled noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe6e7f",
   "metadata": {},
   "source": [
    "A Diffusion Model is a latent variable model which maps to the latent space using a fixed Markov chain. This chain gradually adds noise to the data in order to obtain the approximate posterior $\\{q(\\mathbf{x}_T|\\mathbf{x}_{T-1}), q(\\mathbf{x}_{T-1}|\\mathbf{x}_{T-2}),\\cdots,q(\\mathbf{x}_1|\\mathbf{x}_{0})\\}$, where $\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_T$ are the latent variables with the same dimensionality as$\\mathbf{x}_0$ .\n",
    "\n",
    "Ultimately, the image is asymptotically transformed to pure Gaussian noise. The goal of training a diffusion model is to learn the reverse process - i.e. training $p_\\theta(\\mathbf{x}_{T-1}| \\mathbf{x}_{T})$. By traversing backwards along this chain, we can generate new data.\n",
    "\n",
    "The sampling chain transitions in the forward process can be set to conditional Gaussians when the noise level is sufficiently low. Combining this fact with the Markov assumption leads to a simple parameterization of the forward process:\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_T|\\mathbf{x}_{0})=\\prod_{t=1}^T q(\\mathbf{x}_t|\\mathbf{x}_{t-1}) = \\prod_{t=1}^T  \\mathcal{N}(\\mathbf{x}_t;\\sqrt{1-\\beta}\\mathbf{x}_{t-1},\\beta_t \\mathbf{I})\n",
    "$$\n",
    "\n",
    "A process that is performed at each step in the Markov chain, such that we are simply sampling from a Gaussian distribution whose mean is the previous value (i.e. image) in the chain as $image_t=image_{t-1}+N(0,1)$. Here in this description $\\beta_t$ is a variance schedule (either learned or fixed) which, if well-behaved, ensures that$\\mathbf{x}_t$ is nearly an isotropic Gaussian for sufficiently large T.\n",
    "\n",
    "\n",
    "During training, the model learns to reverse this diffusion process in order to generate new data. Starting with the pure Gaussian noise $p(\\mathbf{x}_T)=\\mathcal{N}(\\mathbf{x}_T;\\mathbf{0},\\mathbf{I})$ the model learns the joint distribution $p_\\theta(\\mathbf{x}_{0:T})$ as:\n",
    "\n",
    "$$\n",
    "p_\\theta(\\mathbf{x}_{0:T})= p(\\mathbf{x}_T)\\prod_{t=1}^T p_\\theta(\\mathbf{x}_{t-1}| \\mathbf{x}_{t}) =\n",
    "p(\\mathbf{x}_T)\\prod_{t=1}^T \\mathcal{N}(\\mathbf{x}_{t-1};\\mathbf{\\mu}_\\theta(\\mathbf{x}_{t},t),\\mathbf{\\Sigma}_\\theta(\\mathbf{x}_{t},t))\n",
    "$$\n",
    "where the time-dependent parameters of the Gaussian transitions are learned. \n",
    "\n",
    "A Diffusion Model is trained by finding the reverse Markov transitions that maximize the likelihood of the training data. In practice, training equivalently consists of minimizing the variational upper bound on the negative log likelihood.\n",
    "\n",
    "$$\n",
    "E\\left[-\\log p_\\theta(\\mathbf{x}_{0}\\right] \\le E_q \\left[ - \\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)}\\right] \\equiv L_{vlb}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389734c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41341e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim parameter specifies the number of feature maps before the first down-sampling, \n",
    "# dim_mults parameter provides multiplicands for this value and successive down-samplings\n",
    "model = Unet(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd23612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the size of images to generate, \n",
    "# the number of timesteps in the diffusion process, \n",
    "# choice between the L1 and L2 norms\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = 128,\n",
    "    timesteps = 1000,   # number of steps\n",
    "    loss_type = 'l1'    # L1 or L2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e582e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = torch.randn(8, 3, 128, 128)\n",
    "loss = diffusion(training_images)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5460e808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510bfbebfb3b431289e5b507f33e80f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampled_images = diffusion.sample(batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n",
    "\n",
    "model = Unet(\n",
    "    dim = 64,\n",
    "    dim_mults = (1, 2, 4, 8)\n",
    ").cuda()\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = 128,\n",
    "    timesteps = 1000,   # number of steps\n",
    "    loss_type = 'l1'    # L1 or L2\n",
    ").cuda()\n",
    "\n",
    "trainer = Trainer(\n",
    "    diffusion,\n",
    "    'path/to/your/images',\n",
    "    train_batch_size = 32,\n",
    "    train_lr = 2e-5,\n",
    "    train_num_steps = 700000,         # total training steps\n",
    "    gradient_accumulate_every = 2,    # gradient accumulation steps\n",
    "    ema_decay = 0.995,                # exponential moving average decay\n",
    "    amp = True                        # turn on mixed precision\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
